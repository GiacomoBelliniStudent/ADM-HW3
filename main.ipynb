{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Homework 3 - Places of the world**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import shutil\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **1. Data collection**\n",
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here we detail the procedure to follow for the data collection."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.1. Get the list of places**\n",
    "We start with the list of places to include in your corpus of documents. In particular, we focus on the Most popular places. Next, we want you to collect the URL associated with each site in the list from this list. The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in the first 400 pages (each page has 18 places, so that you will end up with 7200 unique place URLs).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to the place's URL."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'https://www.atlasobscura.com/places?page={}&sort=likes_count'\n",
    "result = requests.get(url)\n",
    "\n",
    "print(result)\n",
    "print(result.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(result.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mydivs = soup.find_all(\"a\", {\"class\": \"content-card content-card-place\"})\n",
    "'https://www.atlasobscura.com' + mydivs[1]['href']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('places_url.txt', 'w') as f:\n",
    "    for i in range(1,401):\n",
    "        url = 'https://www.atlasobscura.com/places?page={}&sort=likes_count'.format(i)\n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.text)\n",
    "        mydivs = soup.find_all(\"a\", {\"class\": \"content-card content-card-place\"})\n",
    "        for anchor in mydivs:\n",
    "            f.write('https://www.atlasobscura.com' + anchor['href'] + \"\\n\")\n",
    "    f.close();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.2. Crawl places**\n",
    "\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "Download the HTML corresponding to each of the collected URLs.\n",
    "After you collect a single page, immediately save its HTML in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "Organize the entire set of downloaded HTML pages into folders. Each folder will contain the HTML of the places on page 1, page 2, ... of the list of locations.\n",
    "\n",
    "Tip: Due to a large number of pages you should download, you can use some methods that can help you shorten the time it takes. If you employed a particular process or approach, kindly describe it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def saveHtmlFiles():\n",
    "    notDownloadedUrls = []\n",
    "    with open('places_url.txt') as file:\n",
    "        for url in file:\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url,url.split('/')[-1].replace('\\n','')+'.html')\n",
    "            except Exception as e:\n",
    "                notDownloadedUrls.append(url)\n",
    "                continue\n",
    "    file.close()\n",
    "    return notDownloadedUrls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "notDownloadedUrls = saveHtmlFiles()\n",
    "\n",
    "while notDownloadedUrls:\n",
    "    with open('places_url.txt', 'w') as f:\n",
    "        for url in notDownloadedUrls:\n",
    "            f.write(url + \"\\n\")\n",
    "    f.close();\n",
    "    notDownloadedUrls = saveHtmlFiles()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.3 Parse downloaded pages**\n",
    "\n",
    "At this point, you should have all the HTML documents about the places of interest, and you can start to extract the places' information. The list of the information we desire for each place and their format is as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "file_path = os.getcwd()\n",
    "files = []\n",
    "with open('places_url.txt') as file:\n",
    "    for url in file:\n",
    "        files.append(url.split('/')[-1].replace('\\n','')+'.html')\n",
    "    file.close()\n",
    "file_list = np.array_split(files,400)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(400):\n",
    "    directory = 'page_'+ str(i+1)\n",
    "\n",
    "    target_dir = os.path.join(file_path, directory)\n",
    "\n",
    "    os.mkdir(target_dir)\n",
    "\n",
    "    for file_name in file_list[i]:\n",
    "        shutil.move(os.path.join(file_path, file_name), target_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "headerList = ['placeName','placeTags','numPeopleVisited','numPeopleWant','placeDesc','placeShortDesc','placesNearby','placeAddress','placeAlt','placeLong','placeEditors','placePubDate','placeRelatedLists','placeRelatedPlaces','placeUrl']\n",
    "for i in range(1,401):\n",
    "    dir = os.getcwd() + '\\page_{}'.format(i)\n",
    "    files = [f for f in listdir(dir) if isfile(join(dir, f)) and f.lower().endswith(('.html'))]\n",
    "    for file in files:\n",
    "        with open(os.path.join(dir,file), 'rb+') as fp:\n",
    "            soup = BeautifulSoup(fp, \"html.parser\")\n",
    "\n",
    "            placeName = '' if soup.select('h1')[0].text.strip() is None else soup.select('h1')[0].text.strip()\n",
    "\n",
    "            placeTags = '' if soup.find('div', {'class': 'item-tags'}) is None else [a.text.strip() for a in soup.find('div', {'class': 'item-tags'}).select('a', {'class': 'itemTags__link'})]\n",
    "\n",
    "            numPeopleVisited = ('' if soup.find('aside', {'class': 'DDPage__item-actions'}) is None else soup.find('aside', {'class': 'DDPage__item-actions'}).select('div', {'class': 'title-md item-action-count'})[3].text.strip())\n",
    "\n",
    "            numPeopleWant = ('' if soup.find('aside', {'class': 'DDPage__item-actions'}) is None else soup.find('aside', {'class': 'DDPage__item-actions'}).select('div', {'class': 'title-md item-action-count'})[4].text.strip())\n",
    "\n",
    "            numPeopleWant = '' if numPeopleVisited == '' else int(''.join(filter(str.isdigit, numPeopleVisited)))\n",
    "\n",
    "            placeDesc = '' if soup.find('div', {'id': 'place-body'}) is None else soup.find('div', {'id': 'place-body'}).findNext().text.strip()\n",
    "\n",
    "            placeShortDesc = '' if soup.find('h3', {'class': 'DDPage__header-dek'}) is None else soup.find('h3', {'class': 'DDPage__header-dek'}).text.strip()\n",
    "\n",
    "            placesNearby = '' if soup.findAll('div', {'class': 'DDPageSiderailRecirc__item-title'}) is None else set([a.text.strip() for a  in soup.findAll('div', {'class': 'DDPageSiderailRecirc__item-title'})])\n",
    "\n",
    "            placeAddress = '' if soup.find('address', {'class': 'DDPageSiderail__address'}) is None else soup.find('address', {'class': 'DDPageSiderail__address'}).text.strip()\n",
    "\n",
    "            placeAlt = '' if soup.find('div', {'class': 'DDPageSiderail__coordinates'}) is None else soup.find('div', {'class': 'DDPageSiderail__coordinates'}).text.strip().split(',')[0].strip()\n",
    "\n",
    "            placeLong = '' if soup.find('div', {'class': 'DDPageSiderail__coordinates'}) is None else soup.find('div', {'class': 'DDPageSiderail__coordinates'}).text.strip().split(',')[1].strip()\n",
    "\n",
    "            placeEditors = '' if soup.findAll('a', {'class': 'DDPContributorsList__contributor'}) is None else [ a.text.strip() for a in soup.findAll('a', {'class': 'DDPContributorsList__contributor'})]\n",
    "\n",
    "            placePubDate = '' if soup.find('div', {'class': 'DDPContributor__name'}) is None else datetime.strptime(soup.find('div', {'class': 'DDPContributor__name'}).text,'%B %d, %Y').date()\n",
    "\n",
    "            placeRelatedLists = '' if soup.find('div', {'class': 'CardRecircSection__title'}, text='Related Places') is None else ([a.findNext('span').text.strip() for a in soup.find('div', {'class': 'CardRecircSection__title'}, text='Related Places').findNext('div', {'class': 'CardRecircSection__card-grid'}).findAll('a')])\n",
    "\n",
    "            placeRelatedPlaces = '' if soup.find('div', {'class': 'CardRecircSection__title'}, text='Appears in') is None else ([a.findNext('span').text.strip() for a in soup.find('div', {'class': 'CardRecircSection__title'}, text='Appears in').findNext('div', {'class': 'CardRecircSection__card-grid'}).findAll('a')])\n",
    "\n",
    "            placeUrl = '' if soup.find('div', {'class': 'DDPageSiderail__website'}) is None else soup.find('div', {'class': 'DDPageSiderail__website'}).find('a')['href'].strip()\n",
    "\n",
    "        fp.close()\n",
    "\n",
    "        valuesList = [placeName,placeTags,numPeopleVisited,numPeopleWant,placeDesc,placeShortDesc,placesNearby,placeAddress,placeAlt,placeLong,placeEditors,placePubDate,placeRelatedLists,placeRelatedPlaces,placeUrl]\n",
    "        with open(os.path.join(dir, file.title().replace('.Html','').lower() +'.tsv'), 'w+', encoding=\"utf-8\") as tsvfile:\n",
    "            writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "            writer.writerow(headerList)\n",
    "            writer.writerow(valuesList)\n",
    "        tsvfile.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "for i in range(1,401):\n",
    "    dir = os.getcwd() + '\\page_{}'.format(i)\n",
    "    target_dir = os.getcwd() + '/tsv_files'\n",
    "    files = [f for f in listdir(dir) if isfile(join(dir, f)) and f.lower().endswith(('.tsv'))]\n",
    "    for file in files:\n",
    "        shutil.move(os.path.join(dir, file.title().lower()), target_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **2. Search Engine**\n",
    "\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the places that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each place by:\n",
    "\n",
    "- Removing stopwords\n",
    "- Removing punctuation\n",
    "- Stemming\n",
    "- Anything else you think it's needed\n",
    "\n",
    "For this purpose, you can use the nltk library."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "tsv_dir = os.getcwd() + '/tsv_files'\n",
    "files = [f for f in listdir(tsv_dir) if isfile(join(tsv_dir, f)) and f.lower().endswith(('.tsv'))]\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(tsv_dir, file.title().lower()), delimiter=\"\\t\")\n",
    "    df['placeName'] = df['placeName'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "    df['placeTags'] = df['placeTags'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "    df['placeDesc'] = df['placeDesc'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "    df['placeShortDesc'] = df['placeShortDesc'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "    df['placesNearby'] = df['placesNearby'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "    df['placeAddress'] = df['placeAddress'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "    df['placeEditors'] = df['placeEditors'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "    df['placeRelatedLists'] = df['placeRelatedLists'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "    df['placeRelatedPlaces'] = df['placeRelatedPlaces'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "    df['placesUrl'] = df['placeUrl'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}